
# üéØ Deep Dive: Should You Use `customerId` as a Feature in Neural Network-Based Machine Learning Models?

Author: *Your Name*  
Date: *Today*  

---

## Introduction

In recommendation systems and personalization models, **customerId** often seems like a natural input feature.

But **should you use it directly** in a neural network model?

The short answer: **It depends**.  
Using customer IDs without care can lead to **overfitting**, **poor generalization**, and a **fragile model**. In this article, we'll explore why, and walk through a full **PyTorch mini-project** demonstrating the pitfalls ‚Äî plus how we can "rescue" a model using **dropout** and **L2 regularization**.

---

## Why `customerId` is a Dangerous Feature (by itself)

### üõë Raw customerId is meaningless
- Customer IDs are **arbitrary** identifiers.
- Neural networks treat numbers as **ordinal** ‚Äî meaning they think a higher ID might mean "more" or "better."
- If treated as a numeric feature, models **misinterpret IDs** as meaningful, causing **nonsense correlations**.

---

### ‚úÖ Proper ways to use `customerId`
- **Embedding Layer**: Map each customerId into a learnable vector.
- **Behavioral Features**: Replace customerId entirely by summarizing customer behavior (purchase counts, recency, categories liked, etc.).
- **Clustering/Segmentation**: Group similar customers and use cluster ID as a feature.

---

### ‚ö†Ô∏è Sparse Data Problem
- In **real-world systems**, most customers have **few interactions**.
- Embedding customerId directly creates **very sparse** representations.
- The model can easily **memorize the training set** but **fail completely** on unseen customers.

---

## Mini Project: Proving It with Code

### 1. Data Simulation

```python
# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

# Simulate data
np.random.seed(42)
torch.manual_seed(42)

num_customers = 1000
num_products = 100
interactions_per_customer = 5

data = []
for customer_id in range(num_customers):
    preferred = np.random.choice(num_products, 3, replace=False)
    for _ in range(interactions_per_customer):
        product_id = np.random.choice(num_products)
        liked = int(product_id in preferred)
        behavior_score = np.random.rand() * (liked + 0.1)
        data.append([customer_id, product_id, behavior_score, liked])

df = pd.DataFrame(data, columns=["customerId", "productId", "behaviorFeature", "label"])
```

---

### 2. Dataset Class

```python
# PyTorch Dataset class
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, df, use_customer_id=True):
        self.customer_ids = torch.tensor(df["customerId"].values, dtype=torch.long)
        self.product_ids = torch.tensor(df["productId"].values, dtype=torch.long)
        self.behavior = torch.tensor(df["behaviorFeature"].values, dtype=torch.float32).unsqueeze(1)
        self.labels = torch.tensor(df["label"].values, dtype=torch.float32).unsqueeze(1)
        self.use_customer_id = use_customer_id

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        if self.use_customer_id:
            return self.customer_ids[idx], self.product_ids[idx], self.behavior[idx], self.labels[idx]
        else:
            return self.product_ids[idx], self.behavior[idx], self.labels[idx]
```

---

### 3. Model Definitions

**Model A (uses customerId directly)**

```python
class WithCustomerId(nn.Module):
    def __init__(self, num_customers, num_products):
        super().__init__()
        self.customer_embed = nn.Embedding(num_customers, 8)
        self.product_embed = nn.Embedding(num_products, 8)
        self.fc = nn.Sequential(
            nn.Linear(8 + 8 + 1, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, customer_id, product_id, behavior_feat):
        ce = self.customer_embed(customer_id)
        pe = self.product_embed(product_id)
        x = torch.cat([ce, pe, behavior_feat], dim=1)
        return self.fc(x)
```

**Model B (behavior-only, no customerId)**

```python
class WithoutCustomerId(nn.Module):
    def __init__(self, num_products):
        super().__init__()
        self.product_embed = nn.Embedding(num_products, 8)
        self.fc = nn.Sequential(
            nn.Linear(8 + 1, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, product_id, behavior_feat):
        pe = self.product_embed(product_id)
        x = torch.cat([pe, behavior_feat], dim=1)
        return self.fc(x)
```

---

### 4. Training and Evaluation

```python
def train_epoch(model, dataloader, optimizer, criterion, use_customer_id):
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        if use_customer_id:
            customer_id, product_id, behavior, label = batch
            pred = model(customer_id, product_id, behavior)
        else:
            product_id, behavior, label = batch
            pred = model(product_id, behavior)
        loss = criterion(pred, label)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * label.size(0)
    return total_loss / len(dataloader.dataset)

def evaluate_model(model, dataloader, use_customer_id):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            if use_customer_id:
                customer_id, product_id, behavior, label = batch
                pred = model(customer_id, product_id, behavior)
            else:
                product_id, behavior, label = batch
                pred = model(product_id, behavior)
            preds.extend(pred.squeeze().tolist())
            labels.extend(label.squeeze().tolist())
    return roc_auc_score(labels, preds)
```

---

### 5. Results: CustomerId Overfits, Behavior Wins

- **Model A (with customerId)** overfits badly.
- **Model B (behavior features)** generalizes far better.

---

### 6. Rescuing CustomerId Embedding

Adding **Dropout** and **L2 Regularization** to the embedding layer:

```python
class WithCustomerIdDropout(nn.Module):
    def __init__(self, num_customers, num_products):
        super().__init__()
        self.customer_embed = nn.Embedding(num_customers, 8)
        self.product_embed = nn.Embedding(num_products, 8)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Sequential(
            nn.Linear(8 + 8 + 1, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, customer_id, product_id, behavior_feat):
        ce = self.customer_embed(customer_id)
        pe = self.product_embed(product_id)
        x = torch.cat([ce, pe, behavior_feat], dim=1)
        x = self.dropout(x)
        return self.fc(x)
```

```python
optimizer_c = optim.Adam(model_c.parameters(), lr=0.01, weight_decay=1e-4)
```

---

# üìö Appendix: Theoretical Analysis

### Why Does Using CustomerId Cause Overfitting?

---

### 1. Setup

Each customer \(i\) is assigned an embedding \(e_i\).

Prediction:

\[
\hat{y}_{ij} = w^\top e_i + b + \epsilon_{ij}
\]

---

### 2. Variance of the Learned Embedding

From classical statistics:

\[
\text{Var}(\hat{e}_i) \propto \frac{1}{n_i}
\]

where \(n_i\) = number of examples for customer \(i\).

Thus:
- Few examples \(\Rightarrow\) large variance
- Many examples \(\Rightarrow\) small variance

---

### 3. Impact on Predictions

Prediction variance also scales as:

\[
\text{Var}(\hat{y}_{ij}) \propto \frac{1}{n_i}
\]

Fewer examples ‚Üí high prediction variance ‚Üí overfitting.

---

### 4. Why Dropout and L2 Help

- **Dropout** prevents embeddings from becoming too "sharp" or memorized.
- **L2 Regularization** keeps embeddings small, preventing overfitting.

---

# üì¢ Final Takeaway

- **Using customerId directly** is **dangerous** unless the user has **lots of data**.
- Prefer **behavioral features** for generalization.
- Use **embedding regularization** if embeddings must be used.

---
